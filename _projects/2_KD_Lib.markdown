---
layout: page
title: KD_Lib
description: A Pytorch Library for Knowledge Distillation
img: /assets/img/kd_proj.png
---

A Pytorch Library to help extend all Knowledge Distillation works. I am the lead for this project. In future the library will help to benchmark for all the research works in this domain using minimalistic code. 

Currently implemented works
- <a href= "https://arxiv.org/pdf/1503.02531.pdf">Distilling the Knowledge in a Neural Network</a>
- <a href= "https://arxiv.org/pdf/1902.03393.pdf">Improved Knowledge Distillation via Teacher Assistant</a>
- <a href= "https://arxiv.org/pdf/1904.05068.pdf">Relational Knowledge Distillation</a>
- <a href= "https://arxiv.org/pdf/1610.09650.pdf">Distilling Knowledge from Noisy Teachers</a>
- <a href= "https://arxiv.org/pdf/1612.03928.pdf">Paying More Attention To The Attention - Improving the Performance of CNNs via Attention Transfer</a>

<div class="social">
  <span class="contacticon center">
    <a href="https://github.com/SforAiDl/KD_Lib" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
  </span>
  <div class="col three caption">
    Github Link to the project.
  </div>
</div>
